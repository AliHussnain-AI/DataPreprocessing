{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "6D1KqPUqzeIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA and Data Preprocessing\n",
        "\n",
        "In this notebook, we will perform exploratory data analysis (EDA) and data preprocessing using the Car Evaluation dataset. This dataset is available from the UCI Machine Learning Repository and contains information about different car features and their evaluation classes. The notebook is structured to guide you step-by-step through loading the data, converting categorical features, and preparing the data for use in PyTorch.\n",
        "\n",
        "## 1. Importing Libraries\n",
        "\n",
        "We start by importing the necessary libraries for data manipulation (`pandas`) and for working with PyTorch (`torch` and related modules).\n",
        "\n"
      ],
      "metadata": {
        "id": "N1lP5hfGvH8L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RTUWoLX9e-c9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Loading the Dataset**\n",
        "The Car Evaluation dataset is available at the UCI repository. We can load it directly into a Pandas DataFrame."
      ],
      "metadata": {
        "id": "o6cC-IB0vOjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the Car Evaluation dataset\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data'\n",
        "\n",
        "# Define column names based on the dataset documentation\n",
        "columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
        "\n",
        "# Read the dataset into a DataFrame\n",
        "car_df = pd.read_csv(url, names=columns)\n",
        "\n",
        "# Display the first few rows of the DataFrame\n",
        "print(car_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpyMpIawgjxd",
        "outputId": "56372bfe-8c84-40ff-82ec-1d07cf948417"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  buying  maint doors persons lug_boot safety  class\n",
            "0  vhigh  vhigh     2       2    small    low  unacc\n",
            "1  vhigh  vhigh     2       2    small    med  unacc\n",
            "2  vhigh  vhigh     2       2    small   high  unacc\n",
            "3  vhigh  vhigh     2       2      med    low  unacc\n",
            "4  vhigh  vhigh     2       2      med    med  unacc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* We read the dataset from the given URL and define meaningful column names based on the dataset's documentation.\n",
        "* The .head() function is used to display the first few rows of the DataFrame."
      ],
      "metadata": {
        "id": "mEQAB-R9vSrs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Creating Feature Matrix and Target Vector**\n",
        "Now, we will split the dataset into two parts:\n",
        "\n",
        "* X: The feature matrix containing the car attributes.\n",
        "* y: The target vector containing the class labels (the evaluation of the car)."
      ],
      "metadata": {
        "id": "DACtAEHhvWgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create feature matrix X and target vector y\n",
        "X = car_df.drop('class', axis=1)  # Features\n",
        "y = car_df['class']                # Target class\n",
        "\n",
        "# Display the shapes of X and y\n",
        "print(\"Feature matrix shape:\", X.shape)\n",
        "print(\"Target vector shape:\", y.shape)\n",
        "\n",
        "# Optionally, display the first few rows of X and y\n",
        "print(X.head())\n",
        "print(y.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sw97UvXLvRTk",
        "outputId": "5713f721-67b9-48b2-e323-f82228c15416"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature matrix shape: (1728, 6)\n",
            "Target vector shape: (1728,)\n",
            "  buying  maint doors persons lug_boot safety\n",
            "0  vhigh  vhigh     2       2    small    low\n",
            "1  vhigh  vhigh     2       2    small    med\n",
            "2  vhigh  vhigh     2       2    small   high\n",
            "3  vhigh  vhigh     2       2      med    low\n",
            "4  vhigh  vhigh     2       2      med    med\n",
            "0    unacc\n",
            "1    unacc\n",
            "2    unacc\n",
            "3    unacc\n",
            "4    unacc\n",
            "Name: class, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* We use .drop() to remove the target column (class) from X.\n",
        "* y contains the car evaluations.\n",
        "* We display the shape of the feature matrix and target vector to understand the structure."
      ],
      "metadata": {
        "id": "k_gJvpLbvdvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Encoding Categorical Features**\n",
        "Since all of the features in the dataset are categorical, we need to convert them into numerical format before using them in a machine learning model. We use one-hot encoding with pd.get_dummies() for this purpose."
      ],
      "metadata": {
        "id": "NGyZUITkvpnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical features to numerical values (using pd.get_dummies)\n",
        "X = pd.get_dummies(X, drop_first=True)\n"
      ],
      "metadata": {
        "id": "RRF_ynOpvcWt"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Explanation:**\n",
        "* The get_dummies() function converts categorical columns into multiple binary columns (one-hot encoding). The drop_first=True argument helps avoid multicollinearity by dropping the first category in each feature."
      ],
      "metadata": {
        "id": "aA1_9pYJvu7p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Converting Data to PyTorch Tensors**\n",
        "Next, we convert the processed data into PyTorch tensors for training machine learning models in PyTorch."
      ],
      "metadata": {
        "id": "hWesp53NvywU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(pd.factorize(y)[0], dtype=torch.long)\n",
        "\n",
        "# Display the shapes of the tensors\n",
        "print(\"X tensor shape:\", X_tensor.shape)\n",
        "print(\"y tensor shape:\", y_tensor.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHIN0h_gvtNd",
        "outputId": "e2cffa95-63f0-473a-82bd-0fae91a42505"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X tensor shape: torch.Size([1728, 15])\n",
            "y tensor shape: torch.Size([1728])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* We use torch.tensor() to convert the NumPy arrays into PyTorch tensors.\n",
        "* pd.factorize(y)[0] is used to convert the categorical target labels into numerical values.\n",
        "* The tensor shapes are printed to verify that they match our expectations."
      ],
      "metadata": {
        "id": "gthp0MR9v95E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Creating a Custom Dataset for PyTorch**\n",
        "Now, we will create a custom Dataset class to handle our feature and target data. This class allows us to easily load data in batches during training."
      ],
      "metadata": {
        "id": "nITgweAbwEba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CarEvaluationDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n"
      ],
      "metadata": {
        "id": "CmpKKJxxv8Sn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* The Dataset class requires two methods:\n",
        "* __len__(): Returns the number of samples in the dataset.\n",
        "* __getitem__(): Returns a specific sample (feature, label) at the given index."
      ],
      "metadata": {
        "id": "VvGhPwhEwJBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Creating a DataLoader**\n",
        "We can now create a DataLoader to load data in batches, which is essential for efficient model training."
      ],
      "metadata": {
        "id": "R-2o1WjvwOni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataset\n",
        "car_dataset = CarEvaluationDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 16\n",
        "car_dataloader = DataLoader(car_dataset, batch_size=batch_size, shuffle=True)\n"
      ],
      "metadata": {
        "id": "3cr7UIu2wL-3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Displaying a Batch of Data**\n",
        "Finally, we display a batch of data from the DataLoader to verify that it works as expected."
      ],
      "metadata": {
        "id": "XMGdg6HHwR5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a batch of data\n",
        "for features, labels in car_dataloader:\n",
        "    print(\"Batch features:\\n\", features)\n",
        "    print(\"Batch labels:\\n\", labels)\n",
        "    break  # Only show the first batch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-FVuU47wQg0",
        "outputId": "b114f657-f6da-4aa1-ac94-06651e312a28"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch features:\n",
            " tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.],\n",
            "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
            "        [1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.],\n",
            "        [1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0.],\n",
            "        [0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.],\n",
            "        [1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0.]])\n",
            "Batch labels:\n",
            " tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 3, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary: EDA and Data Preprocessing\n",
        "\n",
        "In this notebook, we explored and preprocessed the Car Evaluation dataset. The main steps covered were:\n",
        "\n",
        "1. **Importing Libraries**: Used `pandas` for data manipulation and `torch` for handling tensors and datasets in PyTorch.\n",
        "\n",
        "2. **Loading the Dataset**: Loaded the Car Evaluation dataset from the UCI repository and assigned appropriate column names.\n",
        "\n",
        "3. **Creating Feature Matrix and Target Vector**: Split the dataset into features (`X`) and target labels (`y`).\n",
        "\n",
        "4. **Encoding Categorical Features**: Applied one-hot encoding to convert categorical features into numerical format using `pd.get_dummies()`.\n",
        "\n",
        "5. **Converting Data to PyTorch Tensors**: Converted the feature matrix and target vector into PyTorch tensors, suitable for machine learning models.\n",
        "\n",
        "6. **Creating a Custom Dataset Class**: Defined a custom PyTorch `Dataset` to handle the feature and label data efficiently.\n",
        "\n",
        "7. **Using DataLoader for Batch Processing**: Created a `DataLoader` to batch the data and shuffle it for training.\n",
        "\n",
        "8. **Displaying a Batch of Data**: Printed a sample batch from the DataLoader to verify the correctness of the preprocessing.\n",
        "\n",
        "This preprocessing prepares the dataset for further steps in building a machine learning model in PyTorch.\n"
      ],
      "metadata": {
        "id": "PGtGGItFwmKL"
      }
    }
  ]
}